Recently, I decided to explore how a traditional Linear Regression model compares to a simple Artificial Neural Network (ANN) when it comes to predicting continuous data. I trained both models on the same dataset to see which one performed better on unseen test data.

The results were quite interesting!

The Linear Regression model did a decent job, giving a Mean Squared Error (MSE) of 55.71 and an RÂ² Score of 0.785. This means it explained about 78.5% of the variation in the target values â€” not bad at all for a basic model.

But when I tested the ANN, which had just two hidden layers, it performed significantly better. It brought down the MSE to 20.33 and boosted the RÂ² Score to 0.921 â€” meaning it explained over 92% of the variation!

Hereâ€™s a quick comparison:

ðŸ”¢ Metric	ðŸ“‰ Linear Regression	ðŸ¤– ANN (Keras)
MSE	55.71	20.33
RÂ² Score	0.785	0.921

ðŸ“‚ Code & Notebook
If you're curious about the implementation, you can check out the full code here:
ðŸ”— GitHub Repository
(Look for the project titled "Regression_ANN_Comparison")

This small experiment showed me how neural networks, even in their simplest form, can uncover deeper patterns that linear models might miss â€” especially when data relationships aren't purely linear. That said, linear models still have their place when interpretability and speed matter.

Always fun to test theories with real data! ðŸ˜Š# Comparing-Linear-Regression-vs.-Neural-Networks
